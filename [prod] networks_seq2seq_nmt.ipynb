{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_kxfdP4hJUPB"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii_vg-XNXTil"
   },
   "source": [
    "## Data Cleaning and Data Preparation \n",
    "\n",
    "You'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "---\n",
    "      May I borrow this book?    ¿Puedo tomar prestado este libro?\n",
    "---\n",
    "\n",
    "\n",
    "There are a variety of languages available, but you'll use the English-Spanish dataset. After downloading the dataset, here are the steps you'll take to prepare the data:\n",
    "\n",
    "\n",
    "1. Add a start and end token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a Vocabulary with word index (mapping from word → id) and reverse word index (mapping from id → word).\n",
    "5. Pad each sentence to a maximum length. (Why? you need to fix the maximum length for the inputs to recurrent encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PvRnGWnvXm6l"
   },
   "outputs": [],
   "source": [
    "def rreplace(s, old, new, occurrence):\n",
    "  li = s.rsplit(old, occurrence)\n",
    "  return new.join(li)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFKB2c_tX4wU"
   },
   "source": [
    "### Define a NMTDataset class with necessary functions to follow Step 1 to Step 4. \n",
    "The ```call()``` will return:\n",
    "1. ```train_dataset```  and ```val_dataset``` : ```tf.data.Dataset``` objects\n",
    "2. ```inp_lang_tokenizer``` and ```targ_lang_tokenizer``` : ```tf.keras.preprocessing.text.Tokenizer``` objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JMAHz7kJXc5N"
   },
   "outputs": [],
   "source": [
    "class NMTDataset:\n",
    "    def __init__(self, problem_type='en-spa'):\n",
    "        self.problem_type = 'en-spa'\n",
    "        self.inp_lang_tokenizer = None\n",
    "        self.targ_lang_tokenizer = None\n",
    "    \n",
    "\n",
    "    def unicode_to_ascii(self, s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    ## Step 1 and Step 2 \n",
    "    def preprocess_sentence(self, w):\n",
    "        w = self.unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "    \n",
    "    def create_dataset(self, path, num_examples):\n",
    "        # path : path to spa-eng.txt file\n",
    "        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
    "        lines = io.open(path, encoding='latin1').read().strip().split('\\n')\n",
    "        word_pairs = [[self.preprocess_sentence(w) for w in rreplace(l,';','\\t',1).split('\\t')]  for l in lines[:num_examples]]\n",
    "        # print(word_pairs)\n",
    "        return zip(*word_pairs)\n",
    "\n",
    "    # Step 3 and Step 4\n",
    "    def tokenize(self, lang):\n",
    "        # lang = list of sentences in a language\n",
    "        \n",
    "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "        lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
    "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
    "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
    "\n",
    "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
    "        ## and pads the sequences to match the longest sequences in the given input\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, lang_tokenizer\n",
    "\n",
    "    def load_dataset(self, path, num_examples=None):\n",
    "        # creating cleaned input, output pairs\n",
    "        inp_lang, targ_lang = self.create_dataset(path, num_examples)\n",
    "\n",
    "        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n",
    "        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "\n",
    "    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\n",
    "        file_path = \"all2.csv\"\n",
    "        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(file_path, num_examples)\n",
    "        \n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EIW4NVBmJ25k"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 256\n",
    "# Let's limit the #training examples for faster training\n",
    "num_examples = 30000\n",
    "\n",
    "dataset_creator = NMTDataset('en-spa')\n",
    "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w2lCTy4vKOkB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([256, 44]), TensorShape([256, 15]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val1 , val2, val3, val4 = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 44), (256, 15)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "\n",
    "### Some important parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "max_length_input = example_input_batch.shape[1]\n",
    "max_length_output = example_target_batch.shape[1]\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "steps_per_epoch = num_examples//BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g-yY9c6aIu1h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length_english, max_length_spanish, vocab_size_english, vocab_size_spanish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44, 15, 2896, 229)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"max_length_english, max_length_spanish, vocab_size_english, vocab_size_spanish\")\n",
    "max_length_input, max_length_output, vocab_inp_size, vocab_tar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "##### \n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    ##-------- LSTM layer in Encoder ------- ##\n",
    "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
    "    return output, h, c\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (256, 44, 1024)\n",
      "Encoder h vecotr shape: (batch size, units) (256, 1024)\n",
      "Encoder c vector shape: (batch size, units) (256, 1024)\n"
     ]
    }
   ],
   "source": [
    "## Test Encoder Stack\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.attention_type = attention_type\n",
    "    \n",
    "    # Embedding Layer\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    #Final Dense layer on which softmax will be applied\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # Define the fundamental cell for decoder recurrent structure\n",
    "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "   \n",
    "\n",
    "\n",
    "    # Sampler\n",
    "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "    # Create attention mechanism with memory = None\n",
    "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                              None, self.batch_sz*[max_length_input], self.attention_type)\n",
    "\n",
    "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
    "\n",
    "    # Define the decoder with respect to fundamental rnn cell\n",
    "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "    \n",
    "  def build_rnn_cell(self, batch_sz):\n",
    "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n",
    "    return rnn_cell\n",
    "\n",
    "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
    "    # ------------- #\n",
    "    # typ: Which sort of attention (Bahdanau, Luong)\n",
    "    # dec_units: final dimension of attention outputs \n",
    "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
    "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
    "\n",
    "    if(attention_type=='bahdanau'):\n",
    "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "    else:\n",
    "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
    "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "    return decoder_initial_state\n",
    "\n",
    "\n",
    "  def call(self, inputs, initial_state):\n",
    "    x = self.embedding(inputs)\n",
    "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DaiO0Z6_Ml1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape:  (256, 14, 229)\n"
     ]
    }
   ],
   "source": [
    "# Test decoder stack\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 'luong')\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
    "\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  # real shape = (BATCH_SIZE, max_length_output)\n",
    "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "  loss = mask* loss\n",
    "  loss = tf.reduce_mean(loss)\n",
    "  return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Bw95utNiFHa"
   },
   "source": [
    "## One train_step operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "\n",
    "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "    real = targ[ : , 1: ]         # ignore <start> token\n",
    "\n",
    "    # Set the AttentionMechanism object with encoder_outputs\n",
    "    decoder.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "    # Create AttentionWrapperState as initial_state for decoder\n",
    "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "    pred = decoder(dec_input, decoder_initial_state)\n",
    "    logits = pred.rnn_output\n",
    "    loss = loss_function(real, logits)\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pey8eb9piMMg"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddefjBMa3jF0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Use tf-addons BasicDecoder for decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate_sentence(sentence):\n",
    "  sentence = dataset_creator.preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = []\n",
    "  for i in sentence.split(' '):\n",
    "    try:\n",
    "        key = inp_lang.word_index[i]\n",
    "        inputs.append(key)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  inference_batch_size = inputs.shape[0]\n",
    "  result = ''\n",
    "\n",
    "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "  dec_h = enc_h\n",
    "  dec_c = enc_c\n",
    "\n",
    "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "  end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "  # Instantiate BasicDecoder object\n",
    "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "  # Setup Memory in decoder stack\n",
    "  decoder.attention_mechanism.setup_memory(enc_out)\n",
    "\n",
    "  # set decoder_initial_state\n",
    "  decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "\n",
    "\n",
    "  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "  \n",
    "  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
    "  return outputs.sample_id.numpy()\n",
    "\n",
    "def translate(sentence):\n",
    "  result = evaluate_sentence(sentence)\n",
    "  #print(result)\n",
    "  result = targ_lang.sequences_to_texts(result)\n",
    "  # print('Input: %s' % (sentence))\n",
    "  # print('Predicted translation: {}'.format(result))\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:Couldn't match files for checkpoint ./training_checkpoints\\ckpt-20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x203d769fb80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WYmYhNN_faR5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\nospa\\AppData\\Local\\Temp/ipykernel_24772/3063832052.py\", line 1, in <module>\n",
      "    translate(u'12-02-2019;/DE COMPUTER TASK GROUP IT SOLUTIONS (CTG IT SOLUTIONS) /MOTIF FRAIS 01/2019,FRAIS 12/2018 /REF 1035388;; � 355,60 ;BNP J')\n",
      "  File \"C:\\Users\\nospa\\AppData\\Local\\Temp/ipykernel_24772/2544752256.py\", line 49, in translate\n",
      "    result = evaluate_sentence(sentence)\n",
      "  File \"C:\\Users\\nospa\\AppData\\Local\\Temp/ipykernel_24772/2544752256.py\", line 45, in evaluate_sentence\n",
      "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1037, in __call__\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 162, in call\n",
      "    return dynamic_decode(\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\typeguard\\__init__.py\", line 927, in wrapper\n",
      "    retval = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 517, in dynamic_decode\n",
      "    res = tf.while_loop(\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 617, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2531, in while_loop_v2\n",
      "    return while_loop(\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2777, in while_loop\n",
      "    loop_vars = body(*loop_vars)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 430, in body\n",
      "    (next_outputs, decoder_state, next_inputs, decoder_finished) = decoder.step(\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\tensorflow_addons\\seq2seq\\basic_decoder.py\", line 183, in step\n",
      "    cell_outputs, cell_state = self.cell(inputs, state, training=training)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1037, in __call__\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\tensorflow_addons\\seq2seq\\attention_wrapper.py\", line 2000, in call\n",
      "    cell_output, next_cell_state = self._cell(cell_inputs, cell_state, **kwargs)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1037, in __call__\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 2469, in call\n",
      "    c, o = self._compute_carry_and_output_fused(z, c_tm1)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 2408, in _compute_carry_and_output_fused\n",
      "    i = self.recurrent_activation(z0)\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 206, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\site-packages\\keras\\activations.py\", line 403, in sigmoid\n",
      "    output = tf.sigmoid(x)\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 206, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 4131, in sigmoid\n",
      "    return gen_math_ops.sigmoid(x, name=name)\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 8955, in sigmoid\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\nospa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\inspect.py\", line 755, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\nospa\\miniconda3\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_24772/3063832052.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtranslate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mu'12-02-2019;/DE COMPUTER TASK GROUP IT SOLUTIONS (CTG IT SOLUTIONS) /MOTIF FRAIS 01/2019,FRAIS 12/2018 /REF 1035388;; � 355,60 ;BNP J'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_24772/2544752256.py\u001B[0m in \u001B[0;36mtranslate\u001B[1;34m(sentence)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mtranslate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m   \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mevaluate_sentence\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m   \u001B[1;31m#print(result)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_24772/2544752256.py\u001B[0m in \u001B[0;36mevaluate_sentence\u001B[1;34m(sentence)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 45\u001B[1;33m   \u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdecoder_instance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdecoder_embedding_matrix\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_tokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstart_tokens\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend_token\u001B[0m\u001B[1;33m=\u001B[0m \u001B[0mend_token\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minitial_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdecoder_initial_state\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     46\u001B[0m   \u001B[1;32mreturn\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample_id\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1036\u001B[0m             self._compute_dtype_object):\n\u001B[1;32m-> 1037\u001B[1;33m           \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1038\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs, initial_state, training, **kwargs)\u001B[0m\n\u001B[0;32m    161\u001B[0m         \u001B[0minit_kwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"initial_state\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minitial_state\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 162\u001B[1;33m         return dynamic_decode(\n\u001B[0m\u001B[0;32m    163\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\typeguard\\__init__.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    926\u001B[0m         \u001B[0mcheck_argument_types\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmemo\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 927\u001B[1;33m         \u001B[0mretval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    928\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\u001B[0m in \u001B[0;36mdynamic_decode\u001B[1;34m(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, training, scope, enable_tflite_convertible, **kwargs)\u001B[0m\n\u001B[0;32m    516\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 517\u001B[1;33m         res = tf.while_loop(\n\u001B[0m\u001B[0;32m    518\u001B[0m             \u001B[0mcondition\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001B[0m in \u001B[0;36mnew_func\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    616\u001B[0m                   if date is None else ('after %s' % date), instructions)\n\u001B[1;32m--> 617\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    618\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001B[0m in \u001B[0;36mwhile_loop_v2\u001B[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001B[0m\n\u001B[0;32m   2530\u001B[0m   \"\"\"\n\u001B[1;32m-> 2531\u001B[1;33m   return while_loop(\n\u001B[0m\u001B[0;32m   2532\u001B[0m       \u001B[0mcond\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcond\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001B[0m in \u001B[0;36mwhile_loop\u001B[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001B[0m\n\u001B[0;32m   2776\u001B[0m       \u001B[1;32mwhile\u001B[0m \u001B[0mcond\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mloop_vars\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2777\u001B[1;33m         \u001B[0mloop_vars\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbody\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mloop_vars\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2778\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mtry_to_pack\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloop_vars\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_basetuple\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\u001B[0m in \u001B[0;36mbody\u001B[1;34m(time, outputs_ta, state, inputs, finished, sequence_lengths)\u001B[0m\n\u001B[0;32m    429\u001B[0m             \"\"\"\n\u001B[1;32m--> 430\u001B[1;33m             (next_outputs, decoder_state, next_inputs, decoder_finished) = decoder.step(\n\u001B[0m\u001B[0;32m    431\u001B[0m                 \u001B[0mtime\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow_addons\\seq2seq\\basic_decoder.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, time, inputs, state, training)\u001B[0m\n\u001B[0;32m    182\u001B[0m         \"\"\"\n\u001B[1;32m--> 183\u001B[1;33m         \u001B[0mcell_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcell_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcell\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    184\u001B[0m         \u001B[0mcell_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpack_sequence_as\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcell_state\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1036\u001B[0m             self._compute_dtype_object):\n\u001B[1;32m-> 1037\u001B[1;33m           \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1038\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow_addons\\seq2seq\\attention_wrapper.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs, state, **kwargs)\u001B[0m\n\u001B[0;32m   1999\u001B[0m         \u001B[0mcell_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcell_state\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2000\u001B[1;33m         \u001B[0mcell_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnext_cell_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_cell\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcell_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcell_state\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2001\u001B[0m         next_cell_state = tf.nest.pack_sequence_as(\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1036\u001B[0m             self._compute_dtype_object):\n\u001B[1;32m-> 1037\u001B[1;33m           \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1038\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs, states, training)\u001B[0m\n\u001B[0;32m   2468\u001B[0m       \u001B[0mz\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_or_size_splits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2469\u001B[1;33m       \u001B[0mc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mo\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_compute_carry_and_output_fused\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc_tm1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2470\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36m_compute_carry_and_output_fused\u001B[1;34m(self, z, c_tm1)\u001B[0m\n\u001B[0;32m   2407\u001B[0m     \u001B[0mz0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mz1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mz2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mz3\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mz\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2408\u001B[1;33m     \u001B[0mi\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecurrent_activation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mz0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2409\u001B[0m     \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecurrent_activation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mz1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    205\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 206\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    207\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\keras\\activations.py\u001B[0m in \u001B[0;36msigmoid\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    402\u001B[0m   \"\"\"\n\u001B[1;32m--> 403\u001B[1;33m   \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msigmoid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    404\u001B[0m   \u001B[1;31m# Cache the logits to use for crossentropy loss.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    205\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 206\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    207\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001B[0m in \u001B[0;36msigmoid\u001B[1;34m(x, name)\u001B[0m\n\u001B[0;32m   4130\u001B[0m     \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"x\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4131\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msigmoid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4132\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001B[0m in \u001B[0;36msigmoid\u001B[1;34m(x, name)\u001B[0m\n\u001B[0;32m   8954\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 8955\u001B[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[0;32m   8956\u001B[0m         _ctx, \"Sigmoid\", name, x)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[0;32m   2063\u001B[0m                         \u001B[1;31m# in the engines. This should return a list of strings.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2064\u001B[1;33m                         \u001B[0mstb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2065\u001B[0m                     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[0;32m   2064\u001B[0m                         \u001B[0mstb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2065\u001B[0m                     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2066\u001B[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001B[0m\u001B[0;32m   2067\u001B[0m                                             value, tb, tb_offset=tb_offset)\n\u001B[0;32m   2068\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1365\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1366\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1367\u001B[1;33m         return FormattedTB.structured_traceback(\n\u001B[0m\u001B[0;32m   1368\u001B[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001B[0;32m   1369\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1265\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmode\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mverbose_modes\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1266\u001B[0m             \u001B[1;31m# Verbose modes need a full traceback\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1267\u001B[1;33m             return VerboseTB.structured_traceback(\n\u001B[0m\u001B[0;32m   1268\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0metype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtb_offset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnumber_of_lines_of_context\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1269\u001B[0m             )\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1122\u001B[0m         \u001B[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1123\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1124\u001B[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001B[0m\u001B[0;32m   1125\u001B[0m                                                                tb_offset)\n\u001B[0;32m   1126\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mformat_exception_as_a_whole\u001B[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[0;32m   1080\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1081\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1082\u001B[1;33m         \u001B[0mlast_unique\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfind_recursion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0morig_etype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1083\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1084\u001B[0m         \u001B[0mframes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat_records\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlast_unique\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mfind_recursion\u001B[1;34m(etype, value, records)\u001B[0m\n\u001B[0;32m    380\u001B[0m     \u001B[1;31m# first frame (from in to out) that looks different.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    381\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mis_recursion_error\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0metype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 382\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    383\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    384\u001B[0m     \u001B[1;31m# Select filename, lineno, func_name to track frames with\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "translate(u'12-02-2019;/DE COMPUTER TASK GROUP IT SOLUTIONS (CTG IT SOLUTIONS) /MOTIF FRAIS 01/2019,FRAIS 12/2018 /REF 1035388;; � 355,60 ;BNP J')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRUuNDeY0HiC"
   },
   "source": [
    "## Use tf-addons BeamSearchDecoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AJ-RTQ0hsJNL"
   },
   "outputs": [],
   "source": [
    "def beam_evaluate_sentence(sentence, beam_width=3):\n",
    "  sentence = dataset_creator.preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = []\n",
    "  for i in sentence.split(' '):\n",
    "      try:\n",
    "          key = inp_lang.word_index[i]\n",
    "          inputs.append(key)\n",
    "      except KeyError:\n",
    "          pass\n",
    "\n",
    "          #inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  inference_batch_size = inputs.shape[0]\n",
    "  result = ''\n",
    "\n",
    "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "  dec_h = enc_h\n",
    "  dec_c = enc_c\n",
    "\n",
    "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "  end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "  # From official documentation\n",
    "  # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
    "  # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
    "  # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
    "  # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
    "\n",
    "  enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "  decoder.attention_mechanism.setup_memory(enc_out)\n",
    "  # print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n",
    "\n",
    "  # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "  hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
    "  decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "  decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "\n",
    "  # Instantiate BeamSearchDecoder\n",
    "  decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
    "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "  # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "  outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
    "  # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n",
    "  # The final beam predictions are stored in outputs.predicted_id\n",
    "  # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n",
    "  # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n",
    "  # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n",
    "\n",
    "  \n",
    "  # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "  # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "  # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
    "  final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
    "  beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "  \n",
    "  return final_outputs.numpy(), beam_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "g_LvXGvX8X-O"
   },
   "outputs": [],
   "source": [
    "def beam_translate_print(sentence):\n",
    "  result, beam_scores = beam_evaluate_sentence(sentence)\n",
    "  print(result.shape, beam_scores.shape)\n",
    "  for beam, score in zip(result, beam_scores):\n",
    "    print(beam.shape, score.shape)\n",
    "    output = targ_lang.sequences_to_texts(beam)\n",
    "    output = [a[:a.index('<end>')] for a in output]\n",
    "    beam_score = [a.sum() for a in score]\n",
    "    print('Input: %s' % (sentence))\n",
    "    for i in range(len(output)):\n",
    "      print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))\n",
    "\n",
    "def beam_translate(sentence, beam_width=3):\n",
    "    result, beam_scores = beam_evaluate_sentence(sentence, beam_width)\n",
    "    #print(result.shape, beam_scores.shape)\n",
    "    for beam, score in zip(result, beam_scores):\n",
    "        #print(beam.shape, score.shape)\n",
    "        output = targ_lang.sequences_to_texts(beam)\n",
    "        output = [a[:a.index('<end>')] for a in output]\n",
    "        beam_score = [a.sum() for a in score]\n",
    "        #print('Input: %s' % (sentence))\n",
    "        #for i in range(len(output)):\n",
    "        #    print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))\n",
    "\n",
    "        res = []\n",
    "        for i in range(len(output)):\n",
    "            res.append((output[i], beam_score[i]))\n",
    "        #return list(map(lambda x: (x, beam_score[x]), output))\n",
    "        #return output\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TODnXBleDzzO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('divers ', -0.010183837),\n",
       " ('amazon ', -10.584278),\n",
       " ('kdo ', -21.618626),\n",
       " ('noel ', -22.015179),\n",
       " ('essence ', -25.999664),\n",
       " ('cine ', -26.196777),\n",
       " ('decathlon ', -26.833103),\n",
       " ('tel ', -26.919903)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beam_translate(u'VIREMENT SEPA EMIS VIREMENT SEPA EMIS /MOTIF LE PTI MARAICHER AM DIFFUSION COMMANDE DU 4 AOUT 2021 /BEN LE PTI MARAICHER AM DIFFUSION /REFDO /REF')\n",
    "\n",
    "beam_translate('11/02/2019;DU 090219 DECATHLON       YUTZ            CARTE   4974XXXXXXXX5145;� 5,00;;BNP J', 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BezQwENFY3L",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert(line):\n",
    "    splitted = line.split(\";\")\n",
    "    if len(splitted) != 6 :\n",
    "        return line\n",
    "\n",
    "        #print(splitted[-1])\n",
    "    #remove last element\n",
    "    del splitted[-1]\n",
    "\n",
    "    #newCategory, score = beam_translate(\";\".join(splitted)).pop()\n",
    "    newCategory = translate(\";\".join(splitted)).pop()\n",
    "    # print(newCategory)\n",
    "    newCategory = newCategory.replace('<end>', '')\n",
    "    #translate(\";\".join(splitted))\n",
    "    #print(newCategory)\n",
    "    splitted.append(newCategory)\n",
    "    # splitted.append(str(score))\n",
    "    return \";\".join(splitted)\n",
    "\n",
    "def handleFile(filename):\n",
    "    lines = io.open(filename, encoding='latin1', errors='ignore').read().split('\\n')\n",
    "    result = \"\"\n",
    "    print('open file '+filename)\n",
    "    for line in lines:\n",
    "        line = convert(line)\n",
    "        result = result + line +\"\\n\"\n",
    "\n",
    "    with open(filename, 'w') as filetowrite:\n",
    "        filetowrite.write(result)\n",
    "        filetowrite.close()\n",
    "        print('write file '+filename)\n",
    "\n",
    "\n",
    "    #   print(os.path.join(root, name\n",
    "\n",
    "\n",
    "#handleFile(\"/content/gdrive/My Drive/notebook/budget/wadmgr/20190502/bnp P/compte cheque E1225419.xls--Sheet0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20210817/bnp J/compte cheque E2292590.xls\n",
      "File 20210817/bnp J/compte cheque E2296890.xls\n",
      "File 20210817/bnp J/compte epargne E2299227.xls\n",
      "File 20210817/bnp J/credit immo E2294406.xls\n",
      "File 20210817/bnp J/ldd E2298182.xls\n",
      "File 20210817/bnp J/pel E2297342.xls\n",
      "File 20210817/bnp J/provisio E2294982.xls\n",
      "File 20210817/bnp P/avenir export_17_08_2021_15_14_56.xls\n",
      "File 20210817/bnp P/cel E2292175.xls\n",
      "File 20210817/bnp P/compte cheque E2295419.xls\n",
      "File 20210817/bnp P/compte cheque E2296890.xls\n",
      "File 20210817/bnp P/compte epargne E2299390.xls\n",
      "File 20210817/bnp P/credit immo E2294406.xls\n",
      "File 20210817/bnp P/ldd E2290970.xls\n",
      "File 20210817/bnp P/livret A E2298572.xls\n",
      "File 20210817/bnp P/pel E2297439.xls\n",
      "File 20210817/bnp P/provisio E2299356.xls\n",
      "File 20210910/BNP J/a venir export_10_09_2021_09_05_58.xls\n",
      "File 20210910/BNP J/compte cheque E2532590.xls\n",
      "File 20210910/BNP J/compte cheque E2536890.xls\n",
      "File 20210910/BNP J/compte epargne E2539227.xls\n",
      "File 20210910/BNP J/credit immo E2534406.xls\n",
      "File 20210910/BNP J/ldd E2538182.xls\n",
      "File 20210910/BNP J/pel E2537342.xls\n",
      "File 20210910/BNP J/provisio E2534982.xls\n",
      "File 20210910/BNP P/a venir export_10_09_2021_09_08_16.xls\n",
      "File 20210910/BNP P/CEL E2532175.xls\n",
      "File 20210910/BNP P/compte cheque E2535419.xls\n",
      "File 20210910/BNP P/compte cheque E2536890.xls\n",
      "File 20210910/BNP P/compte epargne E2539390.xls\n",
      "File 20210910/BNP P/Credit immo E2534406.xls\n",
      "File 20210910/BNP P/LDD E2530970.xls\n",
      "File 20210910/BNP P/Livret A E2538572.xls\n",
      "File 20210910/BNP P/PEL E2537439.xls\n",
      "File 20210910/BNP P/Provisio E2539356.xls\n",
      "File 20211006/BNP J/compte cheque E2792590.xls\n",
      "File 20211006/BNP J/compte cheque E2796890.xls\n",
      "File 20211006/BNP J/compte epargne E2799227.xls\n",
      "File 20211006/BNP J/credit immo E2794406.xls\n",
      "File 20211006/BNP J/ldd E2798182.xls\n",
      "File 20211006/BNP J/pel E2797342.xls\n",
      "File 20211006/BNP P/a venir export_06_10_2021_22_48_02.xls\n",
      "File 20211006/BNP P/cel E2792175.xls\n",
      "File 20211006/BNP P/compte cheque E2795419.xls\n",
      "File 20211006/BNP P/compte cheque E2796890.xls\n",
      "File 20211006/BNP P/compte epargne E2799390.xls\n",
      "File 20211006/BNP P/credit immo E2794406.xls\n",
      "File 20211006/BNP P/ldd E2790970.xls\n",
      "File 20211006/BNP P/livret A E2798572.xls\n",
      "File 20211006/BNP P/pel E2797439.xls\n",
      "File 20211006/BNP P/provisio E2799356.xls\n",
      "File 20211011/BNP J/compte cheque E2842590.xls\n",
      "File 20211011/BNP J/compte cheque E2846890.xls\n",
      "File 20211011/BNP J/compte epargne E2849227.xls\n",
      "File 20211011/BNP J/credit immo E2844406.xls\n",
      "File 20211011/BNP J/ldd E2848182.xls\n",
      "File 20211011/BNP J/pel E2847342.xls\n",
      "File 20211011/BNP P/a venir export_11_10_2021_11_03_24.xls\n",
      "File 20211011/BNP P/cel E2842175.xls\n",
      "File 20211011/BNP P/compte cheque E2845419.xls\n",
      "File 20211011/BNP P/compte cheque E2846890.xls\n",
      "File 20211011/BNP P/compte epargne E2849390.xls\n",
      "File 20211011/BNP P/credit immo E2844406.xls\n",
      "File 20211011/BNP P/ldd E2840970.xls\n",
      "File 20211011/BNP P/livret A E2848572.xls\n",
      "File 20211011/BNP P/pel E2847439.xls\n",
      "File 20211011/BNP P/provisio E2849356.xls\n",
      "File 20211022/BNP J/compte cheque E2952590.xls\n",
      "File 20211022/BNP J/compte cheque E2956890.xls\n",
      "File 20211022/BNP J/compte epargne E2959227.xls\n",
      "File 20211022/BNP J/credit immo E2954406.xls\n",
      "File 20211022/BNP J/ldd E2958182.xls\n",
      "File 20211022/BNP J/pel E2957342.xls\n",
      "File 20211022/BNP P/a venir export_22_10_2021_13_57_12.xls\n",
      "File 20211022/BNP P/cel E2952175.xls\n",
      "File 20211022/BNP P/compte cheque E2955419.xls\n",
      "File 20211022/BNP P/compte cheque E2956890.xls\n",
      "File 20211022/BNP P/compte epargne E2959390.xls\n",
      "File 20211022/BNP P/credit immo E2954406.xls\n",
      "File 20211022/BNP P/ldd E2950970.xls\n",
      "File 20211022/BNP P/livret A E2958572.xls\n",
      "File 20211022/BNP P/pel E2957439.xls\n",
      "File 20211022/BNP P/provisio E2959356.xls\n",
      "File 20211122/BNP J/a venir export_22_11_2021_18_55_42.xls\n",
      "File 20211122/BNP J/compte cheque E3262590.xls\n",
      "File 20211122/BNP J/compte cheque E3266890.xls\n",
      "File 20211122/BNP J/compte epagneE3269227.xls\n",
      "File 20211122/BNP J/credit immo E3264406.xls\n",
      "File 20211122/BNP J/ldd E3268182.xls\n",
      "File 20211122/BNP J/pel E3267342.xls\n",
      "File 20211122/BNP P/a venir export_22_11_2021_18_59_01.xls\n",
      "File 20211122/BNP P/cel E3262175.xls\n",
      "File 20211122/BNP P/compte cheque E3265419.xls\n",
      "File 20211122/BNP P/compte cheque E3266890.xls\n",
      "File 20211122/BNP P/compte epargne E3269390.xls\n",
      "File 20211122/BNP P/credit immo E3264406.xls\n",
      "File 20211122/BNP P/ldd E3260970.xls\n",
      "File 20211122/BNP P/livret A E3268572.xls\n",
      "File 20211122/BNP P/pel E3267439.xls\n",
      "File 20211122/BNP P/provisio E3269356.xls\n",
      "File 20211201/BNP J/compte cheque E3352590.xls\n",
      "File 20211201/BNP J/compte cheque E3356890.xls\n",
      "File 20211201/BNP J/compte epargne E3359227.xls\n",
      "File 20211201/BNP J/credit imm E3354406.xls\n",
      "File 20211201/BNP J/ldd.xls\n",
      "File 20211201/BNP J/pel E3357342.xls\n",
      "File 20211201/BNP P/a venir export_01_12_2021_13_54_55.xls\n",
      "File 20211201/BNP P/cel E3352175.xls\n",
      "File 20211201/BNP P/compte cheque E3356890.xls\n",
      "File 20211201/BNP P/compte chque E3355419.xls\n",
      "File 20211201/BNP P/compte epargne E3359390.xls\n",
      "File 20211201/BNP P/credit immo E3354406.xls\n",
      "File 20211201/BNP P/ldd E3350970.xls\n",
      "File 20211201/BNP P/livret A E3358572.xls\n",
      "File 20211201/BNP P/pel E3357439.xls\n",
      "File 20211201/BNP P/provisio E3359356.xls\n",
      "File 20211227/BNP J/compte cheque E3616890.xls\n",
      "File 20211227/BNP J/compte cheqye E3612590.xls\n",
      "File 20211227/BNP J/compte epargne E3619227.xls\n",
      "File 20211227/BNP J/credit immo E3614406.xls\n",
      "File 20211227/BNP J/ldd E3618182.xls\n",
      "File 20211227/BNP J/pel E3617342.xls\n",
      "File 20211227/BNP P/a venir export_27_12_2021_21_43_51.xls\n",
      "File 20211227/BNP P/cel E3612175.xls\n",
      "File 20211227/BNP P/compte cheque E3615419.xls\n",
      "File 20211227/BNP P/compte cheque E3616890.xls\n",
      "File 20211227/BNP P/compte epargne E3619390.xls\n",
      "File 20211227/BNP P/credit immoE3614406.xls\n",
      "File 20211227/BNP P/ldd E3610970.xls\n",
      "File 20211227/BNP P/livret A E3618572.xls\n",
      "File 20211227/BNP P/pel E3617439.xls\n",
      "File 20211227/BNP P/provisio E3619356.xls\n",
      "File 20220114/BNP J/compte cheque E0142590.xls\n",
      "File 20220114/BNP J/compte cheque E0146890.xls\n",
      "File 20220114/BNP J/compte epargne E0149227.xls\n",
      "File 20220114/BNP J/credit immo E0144406.xls\n",
      "File 20220114/BNP J/ldd E0148182.xls\n",
      "File 20220114/BNP J/pel E0147342.xls\n",
      "File 20220114/BNP P/cel E0142175.xls\n",
      "File 20220114/BNP P/compte cheque E0145419.xls\n",
      "File 20220114/BNP P/compte cheque E0146890.xls\n",
      "File 20220114/BNP P/compte epargne E0149390.xls\n",
      "File 20220114/BNP P/credit immo E0144406.xls\n",
      "File 20220114/BNP P/ldd E0140970.xls\n",
      "File 20220114/BNP P/livret A E0148572.xls\n",
      "File 20220114/BNP P/pel E0147439.xls\n",
      "File 20220114/BNP P/privisio E0149356.xls\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iconv-lite warning: decode()-ing strings is deprecated. Refer to https://github.com/ashtuchkin/iconv-lite/wiki/Use-Buffers-when-decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n",
      "The file was saved!\n"
     ]
    }
   ],
   "source": [
    "!node mgrLite.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file 20220114\\BNP J\\compte cheque E0142590.xls--Sheet0.csv\n",
      "write file 20220114\\BNP J\\compte cheque E0142590.xls--Sheet0.csv\n",
      "open file 20220114\\BNP J\\compte cheque E0146890.xls--Sheet0.csv\n",
      "write file 20220114\\BNP J\\compte cheque E0146890.xls--Sheet0.csv\n",
      "open file 20220114\\BNP J\\compte epargne E0149227.xls--Sheet0.csv\n",
      "write file 20220114\\BNP J\\compte epargne E0149227.xls--Sheet0.csv\n",
      "open file 20220114\\BNP J\\credit immo E0144406.xls--Sheet0.csv\n",
      "write file 20220114\\BNP J\\credit immo E0144406.xls--Sheet0.csv\n",
      "open file 20220114\\BNP J\\ldd E0148182.xls--Sheet0.csv\n",
      "write file 20220114\\BNP J\\ldd E0148182.xls--Sheet0.csv\n",
      "open file 20220114\\BNP J\\pel E0147342.xls--Sheet0.csv\n",
      "write file 20220114\\BNP J\\pel E0147342.xls--Sheet0.csv\n",
      "open file 20220114\\BNP P\\cel E0142175.xls--Sheet0.csv\n",
      "write file 20220114\\BNP P\\cel E0142175.xls--Sheet0.csv\n",
      "open file 20220114\\BNP P\\compte cheque E0145419.xls--Sheet0.csv\n",
      "write file 20220114\\BNP P\\compte cheque E0145419.xls--Sheet0.csv\n",
      "open file 20220114\\BNP P\\compte cheque E0146890.xls--Sheet0.csv\n",
      "write file 20220114\\BNP P\\compte cheque E0146890.xls--Sheet0.csv\n",
      "open file 20220114\\BNP P\\compte epargne E0149390.xls--Sheet0.csv\n",
      "write file 20220114\\BNP P\\compte epargne E0149390.xls--Sheet0.csv\n",
      "open file 20220114\\BNP P\\credit immo E0144406.xls--Sheet0.csv\n",
      "write file 20220114\\BNP P\\credit immo E0144406.xls--Sheet0.csv\n",
      "open file 20220114\\BNP P\\ldd E0140970.xls--Sheet0.csv\n",
      "write file 20220114\\BNP P\\ldd E0140970.xls--Sheet0.csv\n",
      "open file 20220114\\BNP P\\livret A E0148572.xls--Sheet0.csv\n",
      "write file 20220114\\BNP P\\livret A E0148572.xls--Sheet0.csv\n",
      "open file 20220114\\BNP P\\pel E0147439.xls--Sheet0.csv\n",
      "write file 20220114\\BNP P\\pel E0147439.xls--Sheet0.csv\n",
      "open file 20220114\\BNP P\\privisio E0149356.xls--Sheet0.csv\n",
      "write file 20220114\\BNP P\\privisio E0149356.xls--Sheet0.csv\n"
     ]
    }
   ],
   "source": [
    "scanDir = \"20220114\"\n",
    "\n",
    "import os\n",
    "for root, dirs, files in os.walk(scanDir, topdown=False):\n",
    "    for name in files:\n",
    "        if name.endswith(\".csv\"):\n",
    "            absFile = os.path.join(root, name)\n",
    "            handleFile(absFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "networks_seq2seq_nmt.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}